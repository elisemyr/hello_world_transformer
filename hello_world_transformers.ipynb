{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hello World Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device set to use: mps\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Check if we can use GPU (MPS for Mac, CUDA for Linux/Windows)\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Device set to use: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Text Classification\n",
        "\n",
        "Text classification is one of the most common NLP tasks. Let's start with sentiment analysis.\n",
        "\n",
        "### ðŸ“š Question 1: Understanding Pipelines\n",
        "\n",
        "1. What is a pipeline in Hugging Face Transformers? What does it abstract away from the user?\n",
        "2. Visit the pipeline documentation and list at least 3 other tasks (besides text-classification) that are available.\n",
        "3. What happens when you don't specify a model in the pipeline? How can you specify a specific model?\n",
        "\n",
        "### ðŸ“š Question 2: Text Classification Deep Dive\n",
        "\n",
        "1. What is the default model used for text-classification? Look at the output above to find its name, then search for it on the Hugging Face Model Hub.\n",
        "2. What dataset was this model fine-tuned on? What kind of text does it work best with?\n",
        "3. The output includes a score field. What does this score represent? What range of values can it have?\n",
        "4. Challenge: Find a different text-classification model on the Hub that classifies emotions (not just positive/negative). What is its name?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Question 1: Understanding Pipelines**\n",
        "\n",
        "### **1. What is a pipeline?**\n",
        "A pipeline is a ready-to-use Hugging Face tool that bundles all NLP steps into one interface (tokenization, model loading, preprocessing, inference, post-processing).\n",
        "\n",
        "### **What does it abstract away?**\n",
        "- Tokenization  \n",
        "- Model loading  \n",
        "- Preprocessing  \n",
        "- Inference  \n",
        "- Post-processing  \n",
        "All done with:\n",
        "```python\n",
        "pipeline(\"task-name\")\n",
        "2. Other available pipeline tasks\n",
        "\"ner\", \"question-answering\", \"summarization\", \"translation\", \"text-generation\", \"zero-shot-classification\", \"fill-mask\"...\n",
        "\n",
        "3. What if you donâ€™t define a model?\n",
        "A default model is selected. To specify one:\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "Question 2: Text Classification Deep Dive\n",
        "1. Default model\n",
        "distilbert-base-uncased-finetuned-sst-2-english (DistilBERT for sentiment analysis).\n",
        "\n",
        "2. Dataset\n",
        "Fine-tuned on SST-2, a movie review sentiment dataset.\n",
        "Best for short/simple text; struggles with sarcasm or mixed emotions.\n",
        "\n",
        "3. Score meaning\n",
        "Confidence from 0.0â€“1.0; higher is more confident.\n",
        "Binary scores â‰ˆ sum to 1.0.\n",
        "\n",
        "4. Emotion classification models\n",
        "j-hartmann/emotion-english-distilroberta-base\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I love using Transformers!\n",
            "Classification: [{'label': 'POSITIVE', 'score': 0.9994327425956726}]\n"
          ]
        }
      ],
      "source": [
        "# Text Classification - Sentiment Analysis\n",
        "classifier = pipeline(\"text-classification\")\n",
        "\n",
        "text = \"I love using Transformers!\"\n",
        "results = classifier(text)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"Classification:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I'm really excited about learning Transformers!\n",
            "Label: POSITIVE, Score: 0.9997\n",
            "\n",
            "Text: This is terrible and I hate it.\n",
            "Label: NEGATIVE, Score: 0.9995\n",
            "\n",
            "Text: The weather is okay today.\n",
            "Label: POSITIVE, Score: 0.9998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Try with multiple texts\n",
        "texts = [\n",
        "    \"I'm really excited about learning Transformers!\",\n",
        "    \"This is terrible and I hate it.\",\n",
        "    \"The weather is okay today.\"\n",
        "]\n",
        "\n",
        "results = classifier(texts)\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### ðŸ“š Question 3: Named Entity Recognition (NER)\n",
        "\n",
        "1. What does the aggregation_strategy=\"simple\" parameter do in the NER pipeline? Check the token classification documentation.\n",
        "2. Looking at the output above, what do the entity types mean? (ORG, MISC, LOC, PER)\n",
        "3. Why do some words appear with ## prefix (like ##tron and ##icons)? What does this indicate about tokenization\n",
        "4. The model seems to have split \"Megatron\" and \"Decepticons\" incorrectly. Why might this happen? What does this tell you about the model's training data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "1. The default NER model can recognize these entity types:\n",
        "   - PER (Person)\n",
        "   - ORG (Organization)\n",
        "   - LOC (Location)\n",
        "   - MISC (Miscellaneous): other entities\n",
        "   - O (Outside): not an entity\n",
        "\n",
        "2. BIO tagging is a way to label words:\n",
        "   - B-XXX means Beginning of an entity (e.g., B-PER for the first word of a person's name)\n",
        "   - I-XXX means Inside an entity (e.g., I-PER for the rest of a person's name)\n",
        "   - O means Outside \n",
        "\n",
        "3. To extract only specific entities, you can filter the results:\n",
        "   ``` python\n",
        "   persons = [e for e in results if e['entity_group'] == 'PER']\n",
        "   orgs = [e for e in results if e['entity_group'] == 'ORG']\n",
        "   ```\n",
        "\n",
        "4. Yes! You can find multilingual models like `xlm-roberta-base-finetuned-conll03-english` or search the Model Hub for models that support a large range of languages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Apple Inc. was founded by Steve Jobs in Cupertino, California. The company is now led by Tim Cook.\n",
            "\n",
            "Named Entities:\n",
            "Apple Inc: ORG (confidence: 0.9996)\n",
            "Steve Jobs: PER (confidence: 0.9926)\n",
            "Cupertino: LOC (confidence: 0.9768)\n",
            "California: LOC (confidence: 0.9988)\n",
            "Tim Cook: PER (confidence: 0.9997)\n"
          ]
        }
      ],
      "source": [
        "# Named Entity Recognition\n",
        "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. The company is now led by Tim Cook.\"\n",
        "results = ner(text)\n",
        "\n",
        "print(\"Text:\", text)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for entity in results:\n",
        "    print(f\"{entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“š Question 4: Question Answering Systems\n",
        "\n",
        "1. What type of question answering is this? (Extractive vs. Generative) Check the question answering documentation.\n",
        "2. The model outputs start and end indices. What do these represent? Why are they important?\n",
        "3. What is the SQuAD dataset? (Look up the model distilbert-base-cased-distilled-squad on the Hub)\n",
        "4. What happens if the answer is not in the context?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "1. Extractive Q1: Finds and copies the exact answer from the context text. Like highlighting a sentence from a book and Generative QA ->  Creates a new answer, even if it's not written exactly that way in the context. Like writing an answer in your own words.\n",
        "\n",
        "2. The default QA model uses BERT architecture. BERT is good at understanding relationships between words.\n",
        "\n",
        "3. The model looks at every word in the context and decides which word is most likely the start of the answer, which word is most likely the end of the answer, it finds the beginning and end with the highest confidence score.\n",
        "\n",
        "4. If the answer isn't in the context, the model will still try to find something, but the confidence score will be low.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What year were Transformers introduced?\n",
            "Answer: 2017\n",
            "Confidence: 0.989996075630188\n",
            "Answer span: 130-134\n"
          ]
        }
      ],
      "source": [
        "# Question Answering\n",
        "qa = pipeline(\"question-answering\")\n",
        "\n",
        "context = \"\"\"\n",
        "Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" \n",
        "by Vaswani et al. in 2017. They revolutionized natural language processing by using self-attention mechanisms \n",
        "instead of recurrent layers. The Hugging Face library makes it easy to use pre-trained transformer models \n",
        "for various NLP tasks.\n",
        "\"\"\"\n",
        "\n",
        "question = \"What year were Transformers introduced?\"\n",
        "result = qa(question=question, context=context)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", result['answer'])\n",
        "print(\"Confidence:\", result['score'])\n",
        "print(f\"Answer span: {result['start']}-{result['end']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: What paper introduced Transformers?\n",
            "A: Attention is All You Need (confidence: 0.6045)\n",
            "\n",
            "Q: What mechanism do Transformers use instead of recurrent layers?\n",
            "A: self-attention (confidence: 0.5299)\n",
            "\n",
            "Q: What library makes it easy to use transformers?\n",
            "A: Hugging Face (confidence: 0.3635)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Multiple questions\n",
        "questions = [\n",
        "    \"What paper introduced Transformers?\",\n",
        "    \"What mechanism do Transformers use instead of recurrent layers?\",\n",
        "    \"What library makes it easy to use transformers?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = qa(question=question, context=context)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {result['answer']} (confidence: {result['score']:.4f})\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### ðŸ“š Question 5: Text Summarization\n",
        "\n",
        "1. What is the difference between extractive and abstractive summarization? Check the summarization documentation.\n",
        "2. Looking at the code in the next cell, what is the default model used for summarization? Search for it on the Hugging Face Model Hub and determine:Is it an extractive or abstractive model? What architecture does it use? (Hint: look at the model name) What dataset was it trained on?\n",
        "3. What do the max_length and min_length parameters control? What happens if min_length > max_length?\n",
        "4. The parameter clean_up_tokenization_spaces=True is used. What does this parameter do? Why might it be useful for summarization?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ANSWERS\n",
        "1. Extractive summarization picks important sentences directly from the original text and puts them together. Like copying key paragraphs whereas abstractive summarization creates new sentences that capture the main ideas. Like writing a summary in your own words\n",
        "\n",
        "2. The default summarization model uses BART or T5 architecture. These are good at generating new text while keeping the main meaning.\n",
        "\n",
        "3. max_length is the maximum number of words/tokens the summary can have. Longer summaries have more detail and min_length is the minimum number of words/tokens the summary must have. Ensures the summary isn't too short.\n",
        "\n",
        "4. You can find multilingual summarization models on the Model Hub. Models like mBART or mT5 can work with multiple languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text length: 958 characters\n",
            "\n",
            "Summary:\n",
            " Artificial intelligence (AI) has revolutionized numerous industries, from healthcare to finance . As AI becomes more powerful, it's crucial to consider ethical implications and ensure these systems are used responsibly . Natural Language Processing (NLP) is one of the most exciting applications of AI, allowing machines to understand, interpret, and generate human language .\n"
          ]
        }
      ],
      "source": [
        "# Text Summarization\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"\"\"\n",
        "Artificial intelligence (AI) has revolutionized numerous industries, from healthcare to finance, \n",
        "transportation to entertainment. Machine learning, a subset of AI, enables computers to learn \n",
        "from data without being explicitly programmed. Deep learning, in turn, is a subset of machine \n",
        "learning that uses neural networks with multiple layers to model and understand complex patterns.\n",
        "\n",
        "Natural Language Processing (NLP) is one of the most exciting applications of AI, allowing machines \n",
        "to understand, interpret, and generate human language. Recent advances in transformer architectures \n",
        "have led to breakthroughs in machine translation, text generation, and language understanding.\n",
        "\n",
        "These technologies are now being integrated into everyday applications, making our interactions \n",
        "with computers more natural and intuitive. However, as AI becomes more powerful, it's crucial to \n",
        "consider ethical implications and ensure these systems are used responsibly.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarizer(text, max_length=100, min_length=50, do_sample=False)\n",
        "print(\"Original text length:\", len(text), \"characters\")\n",
        "print(\"\\nSummary:\")\n",
        "print(summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### ðŸ“š Question 6: Machine Translation\n",
        "\n",
        "1. What is the architecture behind the Helsinki-NLP/opus-mt-en-de model? Look it up on the Model Hub. What does \"OPUS\" stand for? What does \"MT\" stand for?\n",
        "2. How would you find a model to translate from English to French? Visit the translation documentation and the Model Hub to find at least 2 different models.\n",
        "3. What is the difference between bilingual and multilingual translation models? What are the advantages and disadvantages of each?\n",
        "4. In the code, we specify the task as \"translation_en_to_de\". How does this relate to the model we're loading?\n",
        "5. The output shows a warning about sacremoses. What is this library used for in NLP? Check the MarianMT documentation.\n",
        "6. Challenge: Find a multilingual model (like mBART or M2M100) that can translate between multiple language pairs. How many language pairs does it support?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ANSWERS\n",
        "1. OPUS refers to the dataset/collection whereas OPUS refers to a large set of parallel (translated) texts. MT means Machine Translation. So OPUS-MT = machine translation trained on data from OPUS. \n",
        "2. Helsinki-NLP/opus-mt-en-fr => English to French translation model from OPUS-MT, Helsinki-NLP/opus-mt-tc-big-en-fr => another Englishâ†’French model (a â€œbigâ€ variant) in the same project.\n",
        "3. \n",
        "    - Bilingual model: trained for a single language pair (e.g. English â†’ French) => Pros : often higher translation quality for that pair, simpler, less interference, Cons => you need one model per language pair; many models required to cover many languages.\n",
        "    - Multilingual model: trained to handle many languages and many translation directions with a single model Pros: one model covers many languages, easier to handle many-to-many translation, shares information across languages, Cons: performance may be lower compared to specialized bilingual for a given pair; may be larger and heavier; risk of â€œinterferenceâ€ between languages.\n",
        "\n",
        "4. \"translation_en_to_de\" tells the pipeline that the source language is English and the target is German. If you load a model like opus-mt-en-de, it matches that task so translation works as expected.\n",
        "5. sacremoses is used to split text into tokens or to detokenize text (merge tokens back to readable text). \n",
        "6. mBART-50 supports 50 languages, meaning it can translate between any pair of these, it yields up to 50Ã—49= 2450 directed language pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original (English):\n",
            "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. \n",
            "Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure \n",
            "of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. \n",
            "To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. \n",
            "Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. \n",
            "Sincerely, Bumblebee.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Translation (German):\n",
            "Sehr geehrter Amazon, letzte Woche habe ich eine Optimus Prime Action Figur aus Ihrem Online-Shop in Deutschland bestellt. Leider, als ich das Paket Ã¶ffnete, entdeckte ich zu meinem Entsetzen, dass ich stattdessen eine Action Figur von Megatron geschickt worden war! Als lebenslanger Feind der Decepticons, Ich hoffe, Sie kÃ¶nnen mein Dilemma verstehen. Um das Problem zu lÃ¶sen, Ich fordere einen Austausch von Megatron fÃ¼r die Optimus Prime Figur habe ich bestellt. Eingeschlossen sind Kopien meiner Aufzeichnungen Ã¼ber diesen Kauf. Ich erwarte, von Ihnen bald zu hÃ¶ren. Aufrichtig, Bumblebee.\n"
          ]
        }
      ],
      "source": [
        "# Translation - English to German\n",
        "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. \n",
        "Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure \n",
        "of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. \n",
        "To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. \n",
        "Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. \n",
        "Sincerely, Bumblebee.\"\"\"\n",
        "\n",
        "translator = pipeline(\"translation_en_to_de\", \n",
        "                      model=\"Helsinki-NLP/opus-mt-en-de\")\n",
        "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
        "print(\"Original (English):\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "print(\"Translation (German):\")\n",
        "print(outputs[0]['translation_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try English to French\n",
        "translator_fr = pipeline(\"translation_en_to_fr\", \n",
        "                         model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "short_text = \"Hello, how are you today? I hope you're having a wonderful day.\"\n",
        "translation = translator_fr(short_text, clean_up_tokenization_spaces=True)\n",
        "print(\"English:\", short_text)\n",
        "print(\"French:\", translation[0]['translation_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### ðŸ“š Question 7: Text Generation\n",
        "\n",
        "1. What is the default model used for text generation in the code below? Look it up on the Hub and answer:\n",
        "   - What architecture does GPT-2 use? (decoder-only, encoder-decoder, or encoder-only?)\n",
        "   - How many parameters does the base GPT-2 model have?\n",
        "   - What type of generation does it perform? (autoregressive, non-autoregressive, etc.)\n",
        "2. Why do we use `set_seed(42)` before generation? What would happen without it?\n",
        "3. The code uses `max_length=200`. What other parameters can control text generation? Research and explain:\n",
        "   - `temperature`\n",
        "   - `top_k`\n",
        "   - `do_sample`\n",
        "4. Looking at the output, you can see a warning about truncation. What does this mean? Why is the input being truncated?\n",
        "5. What does `pad_token_id` being set to `eos_token_id` mean? Why is this necessary for GPT-2?\n",
        "6. What are the trade-offs between model size and generation quality?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers\n",
        "\n",
        "\n",
        "\n",
        "### 1. The default model is GPT-2\n",
        "\n",
        "- Architecture: Decoder only \n",
        "- Number of parameters: approx. 124 million \n",
        "- Type of generation: Autoregressive\n",
        "\n",
        "\n",
        "\n",
        "### 2. It makes the output repeatable.  Without it, every time you run the code, the generated text would be different.\n",
        "\n",
        "\n",
        "\n",
        "### 3.\n",
        "- temperature: Controls creativity.  \n",
        "  - Low = safer,more predictable  \n",
        "  - High = more random, more creative  \n",
        "- top_k: Keeps only the*top k most likely words\n",
        "  - Smaller k = safer  \n",
        "  - Larger k =more variety  \n",
        "- do_sample: \n",
        "  - If True => model samples words\n",
        "  - If False => model picks the most likely word each time\n",
        "\n",
        "\n",
        "### 4. The model cuts the input because it is too long  and exceeds GPT 2â€™s maximum token limit.  So the model uses only the allowed number of tokens.\n",
        "\n",
        "\n",
        "### 5. GPT-2 has no pad token , so we reuse the end-of-sentence token as padding.  This avoids errors during generation.\n",
        "\n",
        "\n",
        "### 6. \n",
        "- Small models:\n",
        "  - Faster, lighter  \n",
        "  - Lower quality, more mistakes  \n",
        "- Large models: \n",
        "  - Slower, require more memory  \n",
        "  - Much better, more coherent text  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "set_seed(42)  # Set the seed to get reproducible results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. \n",
            "Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure \n",
            "of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. \n",
            "To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. \n",
            "Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. \n",
            "Sincerely, Bumblebee.\n",
            "\n",
            "Customer service response:\n",
            "Dear Bumblebee, I am sorry to hear that your order was mixed up. I did purchase the right size Optimus Prime figure for me, but I did not receive the correct size shipment. I appreciate your understanding as to the issue.\n",
            "\n",
            "Sincerely, Optimus Prime.\n",
            "\n",
            "Customer service response:\n",
            "\n",
            "Dear Optimus Prime, I have been shopping and seeing a lot of Transformers online. \n",
            "\n",
            "This issue is one of my favorite things about the Transformers series. \n",
            "\n",
            "However, I can find no other Transformers figure of this size. \n",
            "\n",
            "This issue of the Transformers series is just as bad as this issue of mine. \n",
            "\n",
            "If you have any other Transformers figures of this size please contact your local shop.\n"
          ]
        }
      ],
      "source": [
        "# Text Generation\n",
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
        "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
        "\n",
        "outputs = generator(prompt, max_length=200, truncation=True)\n",
        "print(outputs[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Temperature = 0.7 (more focused):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of artificial intelligence is at stake. It is not a matter of whether this will happen in the near future, or when, but a question of how we can get there.\n",
            "\n",
            "The answer is that we should be able to build robots capable of doing things we do not have the means to do. Our ability to do so is largely driven by our ability to design new and more powerful machines that are capable of doing these sorts of things. We are all just now discovering that, even if we are able to do everything we want, there are limits to our ability to do them.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "We might want to build robots that are capable of doing things that we don't have the means to do. But what about a computer that can do things we do not have the means to do?\n",
            "\n",
            "That is a much broader question than is often asked, and this raises many different questions.\n",
            "\n",
            "What do we do with AI, and how do we create an artificial intelligence that can do things that we do not have the means to do? What do we do with the power of artificial intelligence, and how do we make AI the kind of AI that makes us all happy?\n",
            "\n",
            "Advertisement\n",
            "\n",
            "The answer to that is very complex.\n",
            "\n",
            "One of the\n",
            "\n",
            "================================================================================\n",
            "Temperature = 1.5 (more creative):\n",
            "The future of artificial intelligence and robotics as a business requires a fundamental break with any kind of system that has held power. That will require the creation of truly disruptive technologies, like machine brains capable of interpreting the feelings made out of a small amount of human intelligence on an animal. Or to use just to show they're capable by simply showing their age would simply be enough proof. But if even such artificial agents can make you wait, they're not the end of it, either.) No business like computing or robotics will need to adopt one of these models. By and large these things will benefit everyone and all, not just the majority class; the middle class will have a decent chance when they choose instead to join them from any degree.\n",
            "\n",
            "Of course, this point would imply either a more rigid separation between developers and market-leaders, a lack of creativity on the creative edge â€” or they will soon end up being one and the same in society as all robots start out from scratch: As soon as any system emerges that tries to fit everything into no single set of rules, they become all that is good and enough to provide support from, let alone compete financially for a monopoly. Of course nobody ever would wish they could get on anything any longer when it's the government giving them it in the early\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different generation parameters\n",
        "prompt = \"The future of artificial intelligence\"\n",
        "\n",
        "# Different temperature settings\n",
        "print(\"=\"*80)\n",
        "print(\"Temperature = 0.7 (more focused):\")\n",
        "output1 = generator(prompt, max_length=100, temperature=0.7, do_sample=True, truncation=True)\n",
        "print(output1[0]['generated_text'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Temperature = 1.5 (more creative):\")\n",
        "output2 = generator(prompt, max_length=100, temperature=1.5, do_sample=True, truncation=True)\n",
        "print(output2[0]['generated_text'])\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
